{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LeNet-5\n",
    "    first arch, 2 CNN and 3 full layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (avg1): AdaptiveAvgPool2d(output_size=6)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (avg2): AdaptiveAvgPool2d(output_size=16)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self,in_channels = 1,num_classes=10):\n",
    "        super(LeNet5,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels,6,kernel_size = 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avg1 = nn.AdaptiveAvgPool2d(6)\n",
    "        self.conv2 = nn.Conv2d(6,16,kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.avg2 = nn.AdaptiveAvgPool2d(16)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,num_classes)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avg1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avg2(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "      \n",
    "lenet5 = LeNet5() \n",
    "print(lenet5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNNet\n",
    "    used GPU and ReLU first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1000])\n"
     ]
    }
   ],
   "source": [
    "class AlexNNet(nn.Module):\n",
    "    def __init__(self,in_channels=3,num_classes=1000):\n",
    "        super(AlexNNet,self).__init__()\n",
    "        # 11 3 5 4 6 3\n",
    "        self.conv1 = nn.Conv2d(in_channels, 96, kernel_size=11, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6,4096)\n",
    "        self.fc2 = nn.Linear(4096,4096)\n",
    "        self.fc3 = nn.Linear(4096,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "y = torch.randn(BATCH_SIZE, 3, 227, 227)\n",
    "alex_net = AlexNNet()\n",
    "print(alex_net(y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "    something about growth rate\n",
    "    broke my brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(growth_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = F.relu(out)\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([DenseLayer(in_channels + i*growth_rate, growth_rate) for i in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.avg_pool(out)\n",
    "        return out\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_classes, growth_rate=32, block_config=(6, 12, 24, 16), compression=0.5):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 2*growth_rate, kernel_size=7, stride=2, padding=3)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(2*growth_rate)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "\n",
    "        in_channels = 2*growth_rate\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock(in_channels, growth_rate, num_layers)\n",
    "            setattr(self, f'denseblock{i+1}', block)\n",
    "            in_channels += num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = TransitionLayer(in_channels, int(in_channels * compression))\n",
    "                setattr(self, f'transition{i+1}', trans)\n",
    "                in_channels = int(in_channels * compression)\n",
    "        \n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            out = getattr(self, f'denseblock{i}')(out)\n",
    "            if i != 4:\n",
    "                out = getattr(self, f'transition{i}')(out)\n",
    "        \n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def DenseNet121():\n",
    "    BATCH_SIZE = 5\n",
    "    y = torch.randn(BATCH_SIZE, 3, 227, 227)\n",
    "    model = DenseNet(num_classes=10, growth_rate=32, block_config=(6, 12, 24, 16), compression=0.5)\n",
    "    print(model(y).shape)\n",
    "\n",
    "DenseNet121()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
