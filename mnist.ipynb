{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels: {0, 1, 2, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "all_labels.extend(train_labels.tolist())\n",
    "unique_labels = set(all_labels)\n",
    "print(\"All labels:\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbtUlEQVR4nO3df2zU9R3H8deBcKC2h7W015MfFlRwQjFDqFVElIa2Wwwocf7McPFHwGImzB/pMgG3JXUs2Yhbh8uy0LkB/sqASBYWLLZkW8FQQWa2NZR1UkNbBhl3pdDC2s/+IN48KeD3uOu7d30+kk9Cv9/vu983H7/05ffu28/5nHNOAAD0syHWDQAABicCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYus27gi3p7e3X48GFlZGTI5/NZtwMA8Mg5p46ODoVCIQ0Zcv77nAEXQIcPH9bYsWOt2wAAXKKWlhaNGTPmvPsH3EtwGRkZ1i0AABLgYj/PkxZAVVVVuvbaazVixAgVFhbqgw8++FJ1vOwGAOnhYj/PkxJAb775ppYvX66VK1fqww8/1LRp01RSUqIjR44k43QAgFTkkmDmzJmuvLw8+nVPT48LhUKusrLyorXhcNhJYjAYDEaKj3A4fMGf9wm/Azp9+rQaGhpUXFwc3TZkyBAVFxervr7+nOO7u7sViURiBgAg/SU8gI4ePaqenh7l5ubGbM/NzVVbW9s5x1dWVioQCEQHT8ABwOBg/hRcRUWFwuFwdLS0tFi3BADoBwn/PaDs7GwNHTpU7e3tMdvb29sVDAbPOd7v98vv9ye6DQDAAJfwO6Dhw4dr+vTpqqmpiW7r7e1VTU2NioqKEn06AECKSspKCMuXL9eiRYt0yy23aObMmVqzZo06Ozv1rW99KxmnAwCkoKQE0AMPPKB///vfWrFihdra2nTzzTdr27Zt5zyYAAAYvHzOOWfdxOdFIhEFAgHrNgAAlygcDiszM/O8+82fggMADE4EEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATFxm3QCAwSsjI8NzzSuvvOK5ZsmSJZ5r4lVbW+u55u677058IymAOyAAgAkCCABgIuEBtGrVKvl8vpgxefLkRJ8GAJDikvIe0E033aT33nvv/ye5jLeaAACxkpIMl112mYLBYDK+NQAgTSTlPaADBw4oFAppwoQJeuSRR3To0KHzHtvd3a1IJBIzAADpL+EBVFhYqOrqam3btk1r165Vc3Oz7rjjDnV0dPR5fGVlpQKBQHSMHTs20S0BAAaghAdQWVmZ7r//fhUUFKikpER/+MMfdPz4cb311lt9Hl9RUaFwOBwdLS0tiW4JADAAJf3pgFGjRumGG25QU1NTn/v9fr/8fn+y2wAADDBJ/z2gEydO6ODBg8rLy0v2qQAAKSThAfTcc8+prq5O//rXv/SXv/xF9957r4YOHaqHHnoo0acCAKSwhL8E9+mnn+qhhx7SsWPHNHr0aM2aNUu7du3S6NGjE30qAEAK8znnnHUTnxeJRBQIBKzbQJJMmjTJc82jjz6ahE76dvToUc8169ev75fzpKM1a9Z4rnnmmWcS30gC9fT0eK5ZtmyZ55qqqirPNf0tHA4rMzPzvPtZCw4AYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJpH8gHQY+n88XV11paannmp///Oeea6699lrPNf3pm9/8pueamTNneq6JZ5HLeI0YMcJzzcqVKz3XLFmyxHPNQDd06FDPNbfccksSOhn4uAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhgNWzoiSeeiKvutddeS3Anqenmm2/2XPPSSy95rlm1apXnmnjV1NR4rrn11luT0EnqOXnypOeajz76KAmdDHzcAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDBYqRpxu/3e655+umnk9AJLmT8+PH9cp6ysrK46saNG5fgTgaPY8eOea5Zs2ZN4htJAdwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMFipGlm6dKlnmsKCgqS0Aku5J///Ge/nOeJJ56Iqy4UCiW4k9R05swZzzWvvPJKEjpJT9wBAQBMEEAAABOeA2jnzp265557FAqF5PP5tHnz5pj9zjmtWLFCeXl5GjlypIqLi3XgwIFE9QsASBOeA6izs1PTpk1TVVVVn/tXr16tV199Va+99pp2796tK664QiUlJerq6rrkZgEA6cPzQwhlZWXn/ZRF55zWrFmj733ve5o/f74k6fXXX1dubq42b96sBx988NK6BQCkjYS+B9Tc3Ky2tjYVFxdHtwUCARUWFqq+vr7Pmu7ubkUikZgBAEh/CQ2gtrY2SVJubm7M9tzc3Oi+L6qsrFQgEIiOsWPHJrIlAMAAZf4UXEVFhcLhcHS0tLRYtwQA6AcJDaBgMChJam9vj9ne3t4e3fdFfr9fmZmZMQMAkP4SGkD5+fkKBoOqqamJbotEItq9e7eKiooSeSoAQIrz/BTciRMn1NTUFP26ublZ+/btU1ZWlsaNG6dnn31WP/zhD3X99dcrPz9fL730kkKhkBYsWJDIvgEAKc5zAO3Zs0d33XVX9Ovly5dLkhYtWqTq6mq98MIL6uzs1FNPPaXjx49r1qxZ2rZtm0aMGJG4rgEAKc/nnHPWTXxeJBJRIBCwbiNl/fa3v/Vc8/DDDyehk8Hj6NGjnmvmzZvnueajjz7yXPPJJ594rpGkMWPGxFWXbt544w3PNY888kgSOklN4XD4gu/rmz8FBwAYnAggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJjx/HAP6TzyrghcXFyehk8Gjo6PDc83rr7/uuSaela1nzZrlueaqq67yXJOOXnzxxbjq1qxZk9hGEIM7IACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZYjHQA+8Y3vuG5JicnJwmdpJ66urq46p5//nnPNQ0NDZ5rRo8e7blm69atnmuuuOIKzzXpaP78+XHV7dixw3PNhx9+GNe5BiPugAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhgMdIBrKmpybqFlHXnnXfGVbd9+3bPNfEsPnnzzTd7rsnMzPRc45zzXJOObrvttrjqZsyY4bmGxUi/PO6AAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmPC5AbZaYSQSUSAQsG5jQMjLy/Nc89e//tVzTVZWluca9D+fz+e5ZoD98zbzwQcfxFU3a9YszzX//e9/4zpXOgqHwxdcRJc7IACACQIIAGDCcwDt3LlT99xzj0KhkHw+nzZv3hyz/7HHHpPP54sZpaWlieoXAJAmPAdQZ2enpk2bpqqqqvMeU1paqtbW1ujYuHHjJTUJAEg/nj8RtaysTGVlZRc8xu/3KxgMxt0UACD9JeU9oNraWuXk5GjSpElasmSJjh07dt5ju7u7FYlEYgYAIP0lPIBKS0v1+uuvq6amRj/60Y9UV1ensrIy9fT09Hl8ZWWlAoFAdIwdOzbRLQEABiDPL8FdzIMPPhj989SpU1VQUKCJEyeqtrZWc+fOPef4iooKLV++PPp1JBIhhABgEEj6Y9gTJkxQdna2mpqa+tzv9/uVmZkZMwAA6S/pAfTpp5/q2LFjcf1WPwAgfXl+Ce7EiRMxdzPNzc3at2+fsrKylJWVpZdfflkLFy5UMBjUwYMH9cILL+i6665TSUlJQhsHAKQ2zwG0Z88e3XXXXdGvP3v/ZtGiRVq7dq3279+v3/zmNzp+/LhCoZDmzZunH/zgB/L7/YnrGgCQ8jwH0Jw5cy64wOEf//jHS2oI/9fa2uq5ZtmyZZ5rqqurPdcgNaTjYqT/+c9/PNesX78+rnOxsGhysRYcAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBEwj+SG7beeecdzzU33HBDXOd68cUXPdcMHTo0rnOhf8WzivapU6c816xdu9ZzTVVVleeaTz75xHMNko87IACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ8Lp5VB5MoEokoEAhYt4Ev4f777/dc8+ijj3qumTBhguear3zlK55rBjqfz+e5Jt5/3r/61a881yxevDiucyF9hcNhZWZmnnc/d0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMsBgpBrybbrrJc83+/fuT0ImteBYj7erqiutct956q+eadJxzXBoWIwUADEgEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMXGbdAAaXMWPGeK7ZuHFjEjoZHMrLy+OqY2FR9AfugAAAJgggAIAJTwFUWVmpGTNmKCMjQzk5OVqwYIEaGxtjjunq6lJ5ebmuvvpqXXnllVq4cKHa29sT2jQAIPV5CqC6ujqVl5dr165d2r59u86cOaN58+aps7MzesyyZcv07rvv6u2331ZdXZ0OHz6s++67L+GNAwBSm6eHELZt2xbzdXV1tXJyctTQ0KDZs2crHA7r17/+tTZs2KC7775bkrRu3TrdeOON2rVrV1yfsggASE+X9B5QOByWJGVlZUmSGhoadObMGRUXF0ePmTx5ssaNG6f6+vo+v0d3d7cikUjMAACkv7gDqLe3V88++6xuv/12TZkyRZLU1tam4cOHa9SoUTHH5ubmqq2trc/vU1lZqUAgEB1jx46NtyUAQAqJO4DKy8v18ccf64033rikBioqKhQOh6OjpaXlkr4fACA1xPWLqEuXLtXWrVu1c+fOmF8sDAaDOn36tI4fPx5zF9Te3q5gMNjn9/L7/fL7/fG0AQBIYZ7ugJxzWrp0qTZt2qQdO3YoPz8/Zv/06dM1bNgw1dTURLc1Njbq0KFDKioqSkzHAIC04OkOqLy8XBs2bNCWLVuUkZERfV8nEAho5MiRCgQCevzxx7V8+XJlZWUpMzNTzzzzjIqKingCDgAQw1MArV27VpI0Z86cmO3r1q3TY489Jkn66U9/qiFDhmjhwoXq7u5WSUmJfvGLXySkWQBA+vA555x1E58XiUQUCASs20CS7N2713NNQUFBEjpJPfEsEHrbbbfFda5Tp07FVQd8XjgcVmZm5nn3sxYcAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBEXJ+ICkjSqlWrPNdMnTo18Y0MEq+++qrnGla1xkDGHRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATLEaKuN14442ea3w+XxI6GRw6OjqsWwASijsgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJliMFHHbuXOn55qTJ08moZNzjR8/Pq66O++8M8GdJM4777xj3QKQUNwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMOFzzjnrJj4vEokoEAhYtwEAuEThcFiZmZnn3c8dEADABAEEADDhKYAqKys1Y8YMZWRkKCcnRwsWLFBjY2PMMXPmzJHP54sZixcvTmjTAIDU5ymA6urqVF5erl27dmn79u06c+aM5s2bp87OzpjjnnzySbW2tkbH6tWrE9o0ACD1efpE1G3btsV8XV1drZycHDU0NGj27NnR7ZdffrmCwWBiOgQApKVLeg8oHA5LkrKysmK2r1+/XtnZ2ZoyZYoqKiou+DHM3d3dikQiMQMAMAi4OPX09Livf/3r7vbbb4/Z/stf/tJt27bN7d+/3/3ud79z11xzjbv33nvP+31WrlzpJDEYDAYjzUY4HL5gjsQdQIsXL3bjx493LS0tFzyupqbGSXJNTU197u/q6nLhcDg6WlpazCeNwWAwGJc+LhZAnt4D+szSpUu1detW7dy5U2PGjLngsYWFhZKkpqYmTZw48Zz9fr9ffr8/njYAACnMUwA55/TMM89o06ZNqq2tVX5+/kVr9u3bJ0nKy8uLq0EAQHryFEDl5eXasGGDtmzZooyMDLW1tUmSAoGARo4cqYMHD2rDhg362te+pquvvlr79+/XsmXLNHv2bBUUFCTlLwAASFFe3vfReV7nW7dunXPOuUOHDrnZs2e7rKws5/f73XXXXeeef/75i74O+HnhcNj8dUsGg8FgXPq42M9+FiMFACQFi5ECAAYkAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJARdAzjnrFgAACXCxn+cDLoA6OjqsWwAAJMDFfp773AC75ejt7dXhw4eVkZEhn88Xsy8SiWjs2LFqaWlRZmamUYf2mIezmIezmIezmIezBsI8OOfU0dGhUCikIUPOf59zWT/29KUMGTJEY8aMueAxmZmZg/oC+wzzcBbzcBbzcBbzcJb1PAQCgYseM+BeggMADA4EEADAREoFkN/v18qVK+X3+61bMcU8nMU8nMU8nMU8nJVK8zDgHkIAAAwOKXUHBABIHwQQAMAEAQQAMEEAAQBMpEwAVVVV6dprr9WIESNUWFioDz74wLqlfrdq1Sr5fL6YMXnyZOu2km7nzp265557FAqF5PP5tHnz5pj9zjmtWLFCeXl5GjlypIqLi3XgwAGbZpPoYvPw2GOPnXN9lJaW2jSbJJWVlZoxY4YyMjKUk5OjBQsWqLGxMeaYrq4ulZeX6+qrr9aVV16phQsXqr293ajj5Pgy8zBnzpxzrofFixcbddy3lAigN998U8uXL9fKlSv14Ycfatq0aSopKdGRI0esW+t3N910k1pbW6PjT3/6k3VLSdfZ2alp06apqqqqz/2rV6/Wq6++qtdee027d+/WFVdcoZKSEnV1dfVzp8l1sXmQpNLS0pjrY+PGjf3YYfLV1dWpvLxcu3bt0vbt23XmzBnNmzdPnZ2d0WOWLVumd999V2+//bbq6up0+PBh3XfffYZdJ96XmQdJevLJJ2Ouh9WrVxt1fB4uBcycOdOVl5dHv+7p6XGhUMhVVlYadtX/Vq5c6aZNm2bdhilJbtOmTdGve3t7XTAYdD/+8Y+j244fP+78fr/buHGjQYf944vz4JxzixYtcvPnzzfpx8qRI0ecJFdXV+ecO/vfftiwYe7tt9+OHvP3v//dSXL19fVWbSbdF+fBOefuvPNO9+1vf9uuqS9hwN8BnT59Wg0NDSouLo5uGzJkiIqLi1VfX2/YmY0DBw4oFAppwoQJeuSRR3To0CHrlkw1Nzerra0t5voIBAIqLCwclNdHbW2tcnJyNGnSJC1ZskTHjh2zbimpwuGwJCkrK0uS1NDQoDNnzsRcD5MnT9a4cePS+nr44jx8Zv369crOztaUKVNUUVGhkydPWrR3XgNuMdIvOnr0qHp6epSbmxuzPTc3V//4xz+MurJRWFio6upqTZo0Sa2trXr55Zd1xx136OOPP1ZGRoZ1eyba2tokqc/r47N9g0Vpaanuu+8+5efn6+DBg/rud7+rsrIy1dfXa+jQodbtJVxvb6+effZZ3X777ZoyZYqks9fD8OHDNWrUqJhj0/l66GseJOnhhx/W+PHjFQqFtH//fr344otqbGzU73//e8NuYw34AML/lZWVRf9cUFCgwsJCjR8/Xm+99ZYef/xxw84wEDz44IPRP0+dOlUFBQWaOHGiamtrNXfuXMPOkqO8vFwff/zxoHgf9ELONw9PPfVU9M9Tp05VXl6e5s6dq4MHD2rixIn93WafBvxLcNnZ2Ro6dOg5T7G0t7crGAwadTUwjBo1SjfccIOampqsWzHz2TXA9XGuCRMmKDs7Oy2vj6VLl2rr1q16//33Yz6+JRgM6vTp0zp+/HjM8el6PZxvHvpSWFgoSQPqehjwATR8+HBNnz5dNTU10W29vb2qqalRUVGRYWf2Tpw4oYMHDyovL8+6FTP5+fkKBoMx10ckEtHu3bsH/fXx6aef6tixY2l1fTjntHTpUm3atEk7duxQfn5+zP7p06dr2LBhMddDY2OjDh06lFbXw8XmoS/79u2TpIF1PVg/BfFlvPHGG87v97vq6mr3t7/9zT311FNu1KhRrq2tzbq1fvWd73zH1dbWuubmZvfnP//ZFRcXu+zsbHfkyBHr1pKqo6PD7d271+3du9dJcj/5yU/c3r173SeffOKcc+6VV15xo0aNclu2bHH79+938+fPd/n5+e7UqVPGnSfWheaho6PDPffcc66+vt41Nze79957z331q191119/vevq6rJuPWGWLFniAoGAq62tda2trdFx8uTJ6DGLFy9248aNczt27HB79uxxRUVFrqioyLDrxLvYPDQ1Nbnvf//7bs+ePa65udlt2bLFTZgwwc2ePdu481gpEUDOOfezn/3MjRs3zg0fPtzNnDnT7dq1y7qlfvfAAw+4vLw8N3z4cHfNNde4Bx54wDU1NVm3lXTvv/++k3TOWLRokXPu7KPYL730ksvNzXV+v9/NnTvXNTY22jadBBeah5MnT7p58+a50aNHu2HDhrnx48e7J598Mu3+J62vv78kt27duugxp06dck8//bS76qqr3OWXX+7uvfde19raatd0ElxsHg4dOuRmz57tsrKynN/vd9ddd517/vnnXTgctm38C/g4BgCAiQH/HhAAID0RQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw8T93cMPl16WcjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128\n",
    "output_size = 10  # 10 classes for digits 0-9\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/simple_nn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Projects\\llm\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0989\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/convNetTest_mnist.pth'))\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = calculate_accuracy(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(128, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Adjusted input size based on conv2 output\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the ConvNet\n",
    "convNet = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = convNet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0734\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./models/cnn_mnist.pth'))\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = calculate_accuracy(convNet, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.924133\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.313336\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678430\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477235\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.526729\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.469570\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.243354\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.535540\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.250727\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.449397\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.415974\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.327603\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.501861\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.146283\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.372229\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.093487\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.173131\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.296608\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.086363\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.334908\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.037923\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.368311\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.237165\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.357118\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.204113\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.050533\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.347196\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.222008\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.196307\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.219461\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.371997\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.076640\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.063637\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.239423\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.055383\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.360913\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.023938\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.091675\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.103321\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.190044\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.072944\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.060187\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.073172\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.164674\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.372170\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.078066\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.129855\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.092128\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.164644\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.046490\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.270464\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.052892\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.031950\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.038460\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.019325\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.053178\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.208981\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.077057\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.056482\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.073632\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.118282\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.053564\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.226998\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.257602\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.246156\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.094773\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.073710\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.141951\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.288561\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.080773\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.124614\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.228794\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.145711\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.069470\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.144412\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.027573\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.028125\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.577887\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.449275\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.129446\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.031079\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.155822\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.013211\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.050209\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.129425\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.036181\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.394779\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.117680\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.209140\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.011749\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.054243\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.511668\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.148937\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.253182\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.080742\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.080144\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.024576\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.009845\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.124462\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.091768\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.060347\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.254630\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.187278\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.014570\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.036776\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.004277\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.159036\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.247975\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.165650\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.020769\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.171206\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.095169\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.146039\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.054088\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.192695\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.040132\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.115668\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.052174\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.456471\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.049946\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.156750\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.024614\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.117552\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.071479\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.227584\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.183102\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.206875\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.161943\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.061622\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.010551\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.049372\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.080375\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.046545\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.237970\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.041292\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.135201\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.067930\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.275516\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.019263\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.171098\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004647\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.216872\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.058772\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.101013\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.065428\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.015367\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.186005\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.195530\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.203482\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.013121\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.094128\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.197300\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.301263\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.125687\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.061686\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.004499\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004558\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.249185\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.025598\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.012293\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.012412\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.014274\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.039375\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.016903\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.035044\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.024567\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.005434\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.157171\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.010556\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001412\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.050422\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.071518\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.043187\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.158121\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.061694\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.042585\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.077480\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.003835\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.071643\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.036640\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.006439\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.098434\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.009693\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.029291\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.028656\n",
      "\n",
      "Test dataset: Overall Loss: 0.0479, Overall Accuracy: 9845/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.075698\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.055071\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.160146\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.095397\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.047407\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.002443\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.063052\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.032045\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.133718\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.014536\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.047828\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.012530\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.097237\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.025002\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.024353\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.032080\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.186925\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.166297\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.098077\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.044796\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.032175\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.062097\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.020596\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.064281\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.024767\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.199381\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.045193\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.015518\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.020066\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.003326\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.037821\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.005519\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.009228\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.002546\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.079078\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.016833\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.012243\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.120761\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.053045\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.061277\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.008337\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.184124\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.034670\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.086315\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.053228\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.025730\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.010458\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.002853\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.097315\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.042974\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.105631\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.164642\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.005448\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.373247\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.038784\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.052734\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.092705\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.096151\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.001856\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.065804\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.159037\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.008316\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.106004\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.016675\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.122087\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.014653\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.078983\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.038760\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.014895\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.004829\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.001739\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.007576\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.165336\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.113387\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.110958\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.022384\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.020373\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.086011\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.047534\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.024318\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.116360\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.018324\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.001088\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.006212\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.059867\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.445822\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.113997\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.003168\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.036717\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.095503\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.008521\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.001779\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.029157\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.070354\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.017105\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.027431\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.027088\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.156768\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.062776\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.009541\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.005300\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.005720\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.015025\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.001287\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.139033\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.010272\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.011606\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.322803\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.004529\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.072160\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.020083\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.027895\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.003890\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.052780\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.090674\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.002149\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.497427\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.005962\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.089726\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.018642\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.023832\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.081781\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.026557\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.006496\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.145347\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.013661\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.224560\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.504058\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.056767\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.073024\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.169390\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.005694\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.009082\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.030739\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.006198\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002554\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.044424\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.228694\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.158914\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.019425\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.003482\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.051014\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.044056\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.051398\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.070854\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.023761\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.130592\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.044132\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.031991\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.031451\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.107672\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.143929\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.037954\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.007448\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.003075\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.004475\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.099167\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.038001\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.003061\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.005109\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.014250\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.173459\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.002985\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.003505\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.099276\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.035642\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.003578\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.126000\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.175342\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.002460\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.426825\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.090889\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.021490\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.075617\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.011608\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.021301\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.000758\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.065469\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.016555\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.027795\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.074182\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.172041\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.064053\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.003793\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.005715\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.017967\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.005941\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.008535\n",
      "\n",
      "Test dataset: Overall Loss: 0.0413, Overall Accuracy: 9865/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ConvoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvoNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op\n",
    "\n",
    "convNetTest=ConvoNet()\n",
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n",
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "model = ConvoNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(convNetTest.state_dict(), './models/convNet_mnist.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
